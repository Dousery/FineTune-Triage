import os
from llama_cpp import Llama

def load_model(model_path, context_size=4096, threads=8):
    """
    GGUF modelini yÃ¼kle
    """
    try:
        print(f"ğŸ”„ Model yÃ¼kleniyor: {os.path.basename(model_path)}")
        
        llm = Llama(
            model_path=model_path,
            n_ctx=context_size,        # context boyutu
            n_threads=threads,         # thread sayÄ±sÄ±
            verbose=False,             # verbose Ã§Ä±ktÄ± kapalÄ±
            n_gpu_layers=0            # CPU kullanÄ±mÄ± iÃ§in 0, GPU iÃ§in artÄ±rÄ±n
        )
        
        print("âœ… Model baÅŸarÄ±yla yÃ¼klendi!")
        return llm
        
    except Exception as e:
        print(f"âŒ Model yÃ¼klenirken hata: {e}")
        return None

def run_inference(llm, prompt, max_tokens=300, temperature=0.7):
    """
    Inference yap
    """
    try:
        response = llm(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            stop=["<|im_end|>"],      # stop token
            echo=False                # prompt'u tekrar gÃ¶sterme
        )
        
        return response['choices'][0]['text'].strip()
        
    except Exception as e:
        print(f"âŒ Inference hatasÄ±: {e}")
        return None

def main():
    """
    Ä°nteraktif chat
    """
    print("ğŸš€ GGUF Model Ä°nteraktif Chat")
    print("=" * 40)
    
    # Model yolunu al
    model_path = input("GGUF model dosyasÄ± yolu (varsayÄ±lan: model.gguf): ").strip()
    if not model_path:
        model_path = "model.gguf"
    
    if not os.path.exists(model_path):
        print(f"âŒ Model dosyasÄ± bulunamadÄ±: {model_path}")
        return
    
    # Modeli yÃ¼kle
    llm = load_model(model_path)
    if not llm:
        return
    
    print("\nğŸ’¬ Ä°nteraktif Chat BaÅŸladÄ±")
    print("Ã‡Ä±kmak iÃ§in 'q' veya 'quit' yazÄ±n")
    print("=" * 40)
    
    while True:
        user_input = input("\nğŸ‘¤ Siz: ").strip()
        
        if user_input.lower() in ['q', 'quit', 'Ã§Ä±k', 'exit']:
            print("ğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!")
            break
        
        if not user_input:
            continue
        
        # Prompt formatla
        prompt = f"""<|im_start|>system
Sen tÄ±bbi aciliyet deÄŸerlendirmesi yapan bir asistansÄ±n.
<|im_end|>
<|im_start|>user
{user_input}
<|im_end|>
<|im_start|>assistant
"""
        
        print("ğŸ”„ DÃ¼ÅŸÃ¼nÃ¼yor...")
        
        response = run_inference(
            llm=llm,
            prompt=prompt,
            max_tokens=300,
            temperature=0.5
        )
        
        if response:
            print(f"ğŸ¤– Asistan: {response}")
        else:
            print("âŒ YanÄ±t alÄ±namadÄ±, tekrar deneyin")

if __name__ == "__main__":
    # Ã–nce kÃ¼tÃ¼phaneyi yÃ¼kle
    try:
        from llama_cpp import Llama
        main()
    except ImportError:
        print("âŒ llama-cpp-python kÃ¼tÃ¼phanesi bulunamadÄ±!")
        print("YÃ¼klemek iÃ§in:")
        print("pip install llama-cpp-python")
        print("\nGPU desteÄŸi iÃ§in:")
        print("pip install llama-cpp-python[cuda]  # NVIDIA GPU")
        print("pip install llama-cpp-python[metal] # Mac M1/M2")

# Kurulum komutlarÄ±:
"""
# CPU versiyonu:
pip install llama-cpp-python

# CUDA GPU desteÄŸi:
pip install llama-cpp-python[cuda]

# Mac Metal desteÄŸi:
pip install llama-cpp-python[metal]

# Manuel derleme ile:
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python
"""